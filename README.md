# Sequence-to-Sequence-Learning-with-Neural-Networks

# Overview
- DNNs cannot be used to map sequences to sequences. This paper addresses this issue and presents a general end-to-end approach to sequence learning using Long Short-Term Memory (LSTM).
- The main result from conducting experiements from the paper is that on an English to French translation task from the WMT'14 dataset.

### About the data
- This dataset has about 700 million words.
- The model has 60000 input words and 80000 output words.
- 4 layers of 1000D cells LSTM, 8000 dimensional state.
- Trains only on 30% of the training data

### Existing problem 

### Approach 

### Results


# Discussion Topic 1
Cover first chosen topic, ask a question for the class to discuss

# Discussion Topic 2
Cover second chosen topic, ask a question for the class to discuss

# Discussion Topic 3
Cover third chosen topic, ask a question for the class to discuss

# Critical Analysis
Answer one or more of the following questions: What was overlooked by the authors? What could have been developed further? Were there any errors? Have others disputed the findings?

# Resource Links
Prepare links of where to go to get more information (other papers, models, blog posts (e.g. papers with code))

# Code Demonstration
Make a Jupyter notebook demonstrating using the model

# Link to Video
Will be uploaded soon.
